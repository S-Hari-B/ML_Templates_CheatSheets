{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98fb94c2-7062-41c5-8c97-4cb5ed6c67e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loading the Datasets for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65f13e9-62ee-4163-a972-2b5db34db20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMDb Example: {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n",
      "\n",
      "AG News Example: {'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n",
      "\n",
      "Twitter Example: {'text': \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", 'date': 'Mon Apr 06 22:19:45 PDT 2009', 'user': '_TheSpecialOne_', 'sentiment': 0, 'query': 'NO_QUERY'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDb (Sentiment Analysis)\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Load AG News (News Classification)\n",
    "ag_news_dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# Load Sentiment140 (Twitter Sentiment)\n",
    "twitter_dataset = load_dataset(\"sentiment140\", trust_remote_code=True)\n",
    "\n",
    "# Print Dataset Samples\n",
    "print(\"\\nIMDb Example:\", imdb_dataset[\"train\"][0])\n",
    "print(\"\\nAG News Example:\", ag_news_dataset[\"train\"][0])\n",
    "print(\"\\nTwitter Example:\", twitter_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05051249-9608-4c48-83eb-d851e7b2d930",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c637db-33cb-4b18-b300-445e66db2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import contractions\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Load SpaCy English model (disable unnecessary components for speed)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "nlp.add_pipe(\"sentencizer\")  # Enable sentence segmentation\n",
    "\n",
    "# Define common informal words & slang\n",
    "INFORMAL_WORDS = {\n",
    "    \"gonna\": \"going to\", \"wanna\": \"want to\", \"gotta\": \"got to\",\n",
    "    \"shoulda\": \"should have\", \"coulda\": \"could have\", \"woulda\": \"would have\",\n",
    "    \"lemme\": \"let me\", \"gimme\": \"give me\", \"outta\": \"out of\",\n",
    "    \"dunno\": \"do not know\", \"kinda\": \"kind of\", \"sorta\": \"sort of\",\n",
    "    \"ain't\": \"is not\", \"ya\": \"you\", \"tho\": \"though\", \"til\": \"until\",\n",
    "    \"cuz\": \"because\", \"coz\": \"because\", \"idk\": \"I do not know\",\n",
    "    \"tbh\": \"to be honest\", \"btw\": \"by the way\", \"u\": \"you\", \"ur\": \"your\",\n",
    "    \"r\": \"are\"\n",
    "}\n",
    "\n",
    "def expand_informal(text):\n",
    "    \"\"\"Replaces informal words using our predefined dictionary.\"\"\"\n",
    "    words = text.split()\n",
    "    words = [INFORMAL_WORDS[word] if word in INFORMAL_WORDS else word for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Stopword list\n",
    "STOPWORDS = nlp.Defaults.stop_words\n",
    "\n",
    "def clean_text(text, remove_punctuation=True, remove_stopwords=True, lemmatize=True, sentence_level=False):\n",
    "    \"\"\"\n",
    "    Cleans raw text by applying several preprocessing steps:\n",
    "    - Lowercasing\n",
    "    - Removing extra spaces\n",
    "    - Informal word expansion\n",
    "    - Punctuation removal (optional)\n",
    "    - URL, email, HTML removal\n",
    "    - Special character normalization\n",
    "    - Stopword removal (optional)\n",
    "    - Lemmatization (optional)\n",
    "    - Sentence segmentation (optional)\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Raw text input.\n",
    "        remove_punctuation (bool): Whether to remove punctuation.\n",
    "        remove_stopwords (bool): Whether to remove stopwords.\n",
    "        lemmatize (bool): Whether to lemmatize tokens.\n",
    "        sentence_level (bool): If True, cleans text at sentence level.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    if text is None or text.strip() == \"\":\n",
    "        return \"\"\n",
    "\n",
    "    # Lowercase text\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Remove URLs, emails, and HTML tags\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Expand informal words\n",
    "    text = expand_informal(text)\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "\n",
    "    # Remove punctuation (if enabled)\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Tokenization using spaCy (Batch Processing)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    if sentence_level:\n",
    "        # Split into sentences and process each one\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            tokens = [\n",
    "                token.lemma_ if lemmatize else token.text\n",
    "                for token in sent\n",
    "                if not (remove_stopwords and token.text in STOPWORDS)\n",
    "            ]\n",
    "            sentences.append(\" \".join(tokens))\n",
    "        return \" \".join(sentences)  # Join cleaned sentences back together\n",
    "    else:\n",
    "        # Process whole text as a single unit\n",
    "        tokens = [\n",
    "            token.lemma_ if lemmatize else token.text\n",
    "            for token in doc\n",
    "            if not (remove_stopwords and token.text in STOPWORDS)\n",
    "        ]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "def process_text_batch(texts, remove_punctuation=True, remove_stopwords=True, lemmatize=True, sentence_level=False, max_jobs=-1):\n",
    "    \"\"\"\n",
    "    Applies text cleaning in parallel to a batch of texts.\n",
    "    \"\"\"\n",
    "    return Parallel(n_jobs=max_jobs)(\n",
    "        delayed(clean_text)(text, remove_punctuation, remove_stopwords, lemmatize, sentence_level) for text in texts\n",
    "    )\n",
    "\n",
    "def apply_cleaning(dataset, columns, batch_size=1000, sentence_level=False, max_jobs=-1):\n",
    "    \"\"\"\n",
    "    Applies text cleaning to either a Hugging Face dataset or a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset, DatasetDict, or pd.DataFrame): The dataset to process.\n",
    "        columns (str or list): Column(s) to clean.\n",
    "        batch_size (int): Batch size for processing.\n",
    "        sentence_level (bool or dict): If True, applies sentence-level cleaning for all columns.\n",
    "                                       If a dict, specify per-column settings (e.g., {\"text\": True, \"summary\": False}).\n",
    "        max_jobs (int): Number of parallel jobs (-1 for max cores).\n",
    "\n",
    "    Returns:\n",
    "        Dataset, DatasetDict, or DataFrame with cleaned text.\n",
    "    \"\"\"\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]  # Convert to list if a single column is given\n",
    "\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        for column in columns:\n",
    "            if column not in dataset.columns:\n",
    "                raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "        cleaned_texts = {col: [] for col in columns}\n",
    "        \n",
    "        for i in tqdm(range(0, len(dataset), batch_size), desc=\"Cleaning DataFrame\"):\n",
    "            batch = dataset.iloc[i:i+batch_size]\n",
    "            for column in columns:\n",
    "                col_sentence_level = sentence_level[column] if isinstance(sentence_level, dict) else sentence_level\n",
    "                cleaned_batch = process_text_batch(batch[column].tolist(), sentence_level=col_sentence_level, max_jobs=max_jobs)\n",
    "                cleaned_texts[column].extend(cleaned_batch)\n",
    "\n",
    "        for column in columns:\n",
    "            dataset[column] = cleaned_texts[column]\n",
    "        return dataset\n",
    "\n",
    "    elif isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        return dataset.map(\n",
    "            lambda batch: {col: [clean_text(text, sentence_level=(sentence_level[col] if isinstance(sentence_level, dict) else sentence_level)) for text in batch[col]] for col in columns}, \n",
    "            batched=True\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type. Use a Hugging Face dataset or pandas DataFrame.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054833c7-d399-4a2f-8d4b-f682ffbbfe4c",
   "metadata": {},
   "source": [
    "### Testing the cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09acb13c-9ed0-47d9-8f08-b84bd5721c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1602ecc50d7421da254d9f5643693ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e912175ee2447db9fa4b54fbdad2815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e6433b63de43f69a7bd35f506db68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00308203b44249e797fd724d0dd40d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMDb Cleaned Example: rent curiousyellow video store controversy surround release 1967 hear seize yous custom try enter country fan film consider controversial myselfthe plot center young swedish drama student name lena want learn life particular want focus attention make sort documentary average swede think certain political issue vietnam war race issue united states ask politician ordinary denizen stockholm opinion politic sex drama teacher classmate marry menwhat kill curiousyellow 40 year ago consider pornographic sex nudity scene far shoot like cheaply porno countryman mind find shock reality sex nudity major staple swedish cinema ingmar bergman arguably answer good old boy john ford sex scene filmsi commend filmmaker fact sex show film show artistic purpose shock people money show pornographic theater america curiousyellow good film want study meat potato pun intend swedish cinema film plot\n",
      "\n",
      "AG News Cleaned Example: wall st bears claw black reuters reuters   shortsellers wall street dwindlingband ultracynic see green\n",
      "\n",
      "Twitter Cleaned Example: switchfoot   awww bummer get david carr day d\n",
      "\n",
      "IMDb (Sentence-Level) Cleaned Example: rent curiousyellow video store controversy surround release 1967 hear seize yous custom try enter country fan film consider controversial myselfthe plot center young swedish drama student lena want learn life particular want focus attention sort documentary average swede think certain political issue vietnam war race issue united states ask politician ordinary denizen stockholm opinion politic sex drama teacher classmate marry menwhat kill curiousyellow 40 year ago consider pornographic sex nudity scene far shoot like cheaply porno countryman mind find shock reality sex nudity major staple swedish cinema ingmar bergman arguably answer good old boy john ford sex scene filmsi commend filmmaker fact sex film artistic purpose shock people money pornographic theater america curiousyellow good film want study meat potato pun intend swedish cinema film plot\n"
     ]
    }
   ],
   "source": [
    "# Load Sample Data\n",
    "sample_imdb = imdb_dataset[\"train\"].select(range(5))\n",
    "sample_ag_news = ag_news_dataset[\"train\"].select(range(5))\n",
    "sample_twitter = twitter_dataset[\"train\"].select(range(5))\n",
    "\n",
    "# Apply Text Cleaning (Word-Level)\n",
    "sample_imdb = apply_cleaning(sample_imdb, columns=\"text\")\n",
    "sample_ag_news = apply_cleaning(sample_ag_news, columns=\"text\")\n",
    "sample_twitter = apply_cleaning(sample_twitter, columns=\"text\")\n",
    "\n",
    "# Apply Text Cleaning to Multiple Columns (Sentence-Level Example)\n",
    "sample_imdb_multi = apply_cleaning(sample_imdb, columns=[\"text\"], sentence_level={\"text\": True})\n",
    "\n",
    "# Print Cleaned Examples\n",
    "print(\"\\nIMDb Cleaned Example:\", sample_imdb[0][\"text\"])\n",
    "print(\"\\nAG News Cleaned Example:\", sample_ag_news[0][\"text\"])\n",
    "print(\"\\nTwitter Cleaned Example:\", sample_twitter[0][\"text\"])\n",
    "print(\"\\nIMDb (Sentence-Level) Cleaned Example:\", sample_imdb_multi[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ec9c9-5452-40d3-b6d4-d335f9f85541",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29bfddd8-e422-4e2b-925f-6dd2d943f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def basic_tokenize(text):\n",
    "    \"\"\"Basic whitespace-based tokenizer as a fallback.\"\"\"\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())  # Simple word tokenization\n",
    "\n",
    "def tokenize_text(dataset, column=\"text\", model=None, max_length=128, sentence_level=False):\n",
    "    \"\"\"\n",
    "    Tokenizes text using either a Hugging Face pre-trained model or a simple tokenizer.\n",
    "    Now includes padding & truncation.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset, DatasetDict, or pd.DataFrame): Dataset to tokenize.\n",
    "        column (str): The text column to process.\n",
    "        model (str or None): Pre-trained tokenizer model (e.g., 'bert-base-uncased'). \n",
    "                             If None, uses a basic tokenizer.\n",
    "        max_length (int): Maximum sequence length for padding/truncation (default: 128).\n",
    "        sentence_level (bool): If True, returns sentence segmentation using spaCy.\n",
    "\n",
    "    Returns:\n",
    "        Dataset, DatasetDict, or DataFrame with tokenized output.\n",
    "    \"\"\"\n",
    "    if model:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)  # Load HF tokenizer\n",
    "    else:\n",
    "        tokenizer = None  # Use basic tokenization instead\n",
    "\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        if column not in dataset.columns:\n",
    "            raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "        def process_text(text):\n",
    "            if tokenizer:\n",
    "                tokenized_output = tokenizer(\n",
    "                    text,\n",
    "                    padding=\"max_length\",  # Pads all sequences to max_length\n",
    "                    truncation=True,       # Cuts off longer sequences\n",
    "                    max_length=max_length, # Defines max sequence length\n",
    "                )\n",
    "                return tokenized_output[\"input_ids\"]  # Returns token IDs\n",
    "            else:\n",
    "                return basic_tokenize(text)  # Simple whitespace tokenization\n",
    "        \n",
    "        dataset[column + \"_tokens\"] = dataset[column].apply(process_text)\n",
    "        return dataset\n",
    "\n",
    "    elif isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        def process_batch(batch):\n",
    "            \"\"\"Handles batch processing to avoid TypeErrors.\"\"\"\n",
    "            texts = batch[column]\n",
    "            if tokenizer:\n",
    "                tokenized_outputs = tokenizer(\n",
    "                    texts,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    return_tensors=None  # Ensure we return Python lists\n",
    "                )\n",
    "                token_ids = tokenized_outputs[\"input_ids\"]\n",
    "                tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in token_ids]\n",
    "            else:\n",
    "                tokens = [basic_tokenize(text) for text in texts]\n",
    "                token_ids = None  # Not needed for basic tokenization\n",
    "\n",
    "            return {\n",
    "                column + \"_tokens\": tokens,\n",
    "                column + \"_token_ids\": token_ids if token_ids else [None] * len(tokens),\n",
    "                column + \"_sentences\": [[sent.text for sent in nlp(text).sents] if sentence_level else None for text in texts]\n",
    "            }\n",
    "\n",
    "        return dataset.map(process_batch, batched=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type. Use a Hugging Face dataset or pandas DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d5486-f08a-42a5-9265-9feb94b574c9",
   "metadata": {},
   "source": [
    "### Testing the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96aba67b-4d47-42a0-a70b-b7a0092e806b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6d99d73aeb4caea86a14a3545842e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007f9e5951bd4b2c853a5656f616cbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15816b61a714414c9226813c5e165d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4141b953aea74b43adc84b82de46a54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMDb Tokenized Example BERT: ['[CLS]', 'rent', 'curious', '##ye', '##llo', '##w', 'video', 'store', 'controversy', 'surround', 'release', '1967', 'hear', 'seize', 'you', '##s', 'custom', 'try', 'enter', 'country', 'fan', 'film', 'consider', 'controversial', 'myself', '##the', 'plot', 'center', 'young', 'swedish', 'drama', 'student', 'name', 'lena', 'want', 'learn', 'life', 'particular', 'want', 'focus', 'attention', 'make', 'sort', 'documentary', 'average', 'sw', '##ede', 'think', 'certain', 'political', 'issue', 'vietnam', 'war', 'race', 'issue', 'united', 'states', 'ask', 'politician', 'ordinary', 'den', '##ize', '##n', 'stockholm', 'opinion', 'pol', '##itic', 'sex', 'drama', 'teacher', 'classmate', 'marry', 'men', '##w', '##hat', 'kill', 'curious', '##ye', '##llo', '##w', '40', 'year', 'ago', 'consider', 'pornographic', 'sex', 'nu', '##dity', 'scene', 'far', 'shoot', 'like', 'cheap', '##ly', 'porn', '##o', 'country', '##man', 'mind', 'find', 'shock', 'reality', 'sex', 'nu', '##dity', 'major', 'staple', 'swedish', 'cinema', 'ing', '##mar', 'bergman', 'arguably', 'answer', 'good', 'old', 'boy', 'john', 'ford', 'sex', 'scene', 'films', '##i', 'com', '##men', '##d', 'filmmaker', '[SEP]']\n",
      "\n",
      "IMDb Tokenized Example Simple: ['rent', 'curiousyellow', 'video', 'store', 'controversy', 'surround', 'release', '1967', 'hear', 'seize', 'yous', 'custom', 'try', 'enter', 'country', 'fan', 'film', 'consider', 'controversial', 'myselfthe', 'plot', 'center', 'young', 'swedish', 'drama', 'student', 'name', 'lena', 'want', 'learn', 'life', 'particular', 'want', 'focus', 'attention', 'make', 'sort', 'documentary', 'average', 'swede', 'think', 'certain', 'political', 'issue', 'vietnam', 'war', 'race', 'issue', 'united', 'states', 'ask', 'politician', 'ordinary', 'denizen', 'stockholm', 'opinion', 'politic', 'sex', 'drama', 'teacher', 'classmate', 'marry', 'menwhat', 'kill', 'curiousyellow', '40', 'year', 'ago', 'consider', 'pornographic', 'sex', 'nudity', 'scene', 'far', 'shoot', 'like', 'cheaply', 'porno', 'countryman', 'mind', 'find', 'shock', 'reality', 'sex', 'nudity', 'major', 'staple', 'swedish', 'cinema', 'ingmar', 'bergman', 'arguably', 'answer', 'good', 'old', 'boy', 'john', 'ford', 'sex', 'scene', 'filmsi', 'commend', 'filmmaker', 'fact', 'sex', 'show', 'film', 'show', 'artistic', 'purpose', 'shock', 'people', 'money', 'show', 'pornographic', 'theater', 'america', 'curiousyellow', 'good', 'film', 'want', 'study', 'meat', 'potato', 'pun', 'intend', 'swedish', 'cinema', 'film', 'plot']\n",
      "\n",
      "AG News Tokenized Example: ['wall', 'st', 'bears', 'claw', 'black', 'reuters', 'reuters', 'shortsellers', 'wall', 'street', 'dwindlingband', 'ultracynic', 'see', 'green']\n",
      "\n",
      "Twitter (Sentence-Level) Example: ['switchfoot   awww bummer get david carr day d']\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization with the updated function\n",
    "sample_imdb_tokenized_model = tokenize_text(sample_imdb, column=\"text\", model=\"bert-base-uncased\")\n",
    "sample_imdb_tokenized_simple = tokenize_text(sample_imdb, column=\"text\", model=None)\n",
    "sample_ag_news_tokenized = tokenize_text(sample_ag_news, column=\"text\")\n",
    "sample_twitter_tokenized = tokenize_text(sample_twitter, column=\"text\", sentence_level=True)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nIMDb Tokenized Example BERT:\", sample_imdb_tokenized_model[0][\"text_tokens\"])\n",
    "print(\"\\nIMDb Tokenized Example Simple:\", sample_imdb_tokenized_simple[0][\"text_tokens\"])\n",
    "print(\"\\nAG News Tokenized Example:\", sample_ag_news_tokenized[0][\"text_tokens\"])\n",
    "print(\"\\nTwitter (Sentence-Level) Example:\", sample_twitter_tokenized[0][\"text_sentences\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a91c0-91ff-4873-873b-ee0a5744e829",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f4f664-c5ec-4242-9ffa-44f20a80fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Load SentenceTransformer Model (small but powerful)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51525fc8-6d6b-4b7b-8bfb-886680a7e156",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd567e9-83f9-4d99-9442-169b8d5bd2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(dataset, column=\"text\", batch_size=32):\n",
    "    \"\"\"\n",
    "    Encodes words individually into embeddings using SentenceTransformers.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset, DatasetDict, or DataFrame): The dataset to process.\n",
    "        column (str): The column containing text.\n",
    "        batch_size (int): Number of sentences per batch.\n",
    "\n",
    "    Returns:\n",
    "        Dataset, DatasetDict, or DataFrame with word embeddings stored.\n",
    "    \"\"\"\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        word_embeddings_dict = {}\n",
    "\n",
    "        def get_word_embeddings(text):\n",
    "            words = text.split()  # Tokenize\n",
    "            word_embeddings = embedding_model.encode(words, batch_size=batch_size, convert_to_numpy=True)\n",
    "            for word, emb in zip(words, word_embeddings):\n",
    "                word_embeddings_dict[word] = emb\n",
    "            return word_embeddings\n",
    "        \n",
    "        # Apply to DataFrame\n",
    "        dataset[column + \"_word_embeddings\"] = dataset[column].apply(get_word_embeddings)\n",
    "        return dataset, word_embeddings_dict  # Return dataset + dictionary for word reuse\n",
    "    \n",
    "    elif isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        word_embeddings_dict = {}\n",
    "\n",
    "        def process_batch(batch):\n",
    "            word_embeddings_batch = []\n",
    "            for text in batch[column]:\n",
    "                words = text.split()\n",
    "                word_embeddings = embedding_model.encode(words, batch_size=batch_size, convert_to_numpy=True)\n",
    "                word_embeddings_batch.append(word_embeddings)\n",
    "                for word, emb in zip(words, word_embeddings):\n",
    "                    word_embeddings_dict[word] = emb\n",
    "            return {column + \"_word_embeddings\": word_embeddings_batch}\n",
    "\n",
    "        dataset = dataset.map(process_batch, batched=True)\n",
    "        return dataset, word_embeddings_dict\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type. Use a Hugging Face dataset or pandas DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88adef-553d-4801-974f-5094e44e209d",
   "metadata": {},
   "source": [
    "### Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b32ef91-21ac-4f5c-8cfd-7c59e312705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(dataset, column=\"text\", model=embedding_model, batch_size=32, num_proc=None):\n",
    "    \"\"\"\n",
    "    Encodes sentences into embeddings using SentenceTransformers.\n",
    "    Now supports parallel processing for Hugging Face datasets.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset, DatasetDict, or DataFrame): The dataset to process.\n",
    "        column (str): The column containing text.\n",
    "        model: Pre-loaded SentenceTransformer model (default: embedding_model).\n",
    "        batch_size (int): Number of sentences per batch.\n",
    "        num_proc (int, optional): Number of processes to use for multiprocessing (Only for Hugging Face datasets).\n",
    "                                  Set `num_proc=-1` to use all available CPU cores.\n",
    "\n",
    "    Returns:\n",
    "        Dataset, DatasetDict, or DataFrame with embeddings stored.\n",
    "    \"\"\"\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        # Convert column to list\n",
    "        sentences = dataset[column].tolist()\n",
    "        # Generate embeddings in batches using the provided model\n",
    "        embeddings = model.encode(sentences, batch_size=batch_size, convert_to_numpy=True)\n",
    "        # Store embeddings\n",
    "        dataset[column + \"_embeddings\"] = list(embeddings)\n",
    "        return dataset\n",
    "\n",
    "    elif isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        # Parallel Processing enabled for Hugging Face Datasets\n",
    "        num_proc = num_proc if num_proc else 1  # Default to 1 process if not set\n",
    "\n",
    "        return dataset.map(\n",
    "            lambda x: {column + \"_embeddings\": model.encode(x[column], batch_size=batch_size, convert_to_numpy=True)},\n",
    "            batched=True,\n",
    "            num_proc=num_proc  # This enables multiprocessing\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type. Use a Hugging Face dataset or pandas DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7306af32-3cc1-4258-a245-37c7f23caf20",
   "metadata": {},
   "source": [
    "### Testing the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c315ede-2bd6-4e46-871a-fb9383d6d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentence-level embeddings\n",
    "sample_imdb_embed = encode_sentences(sample_imdb, column=\"text\")\n",
    "sample_ag_news_embed = encode_sentences(sample_ag_news, column=\"text\")\n",
    "sample_twitter_embed = encode_sentences(sample_twitter, column=\"text\")\n",
    "\n",
    "# Apply word-level embeddings\n",
    "sample_imdb_word_embed, imdb_word_dict = encode_words(sample_imdb, column=\"text\")\n",
    "sample_ag_news_word_embed, ag_news_word_dict = encode_words(sample_ag_news, column=\"text\")\n",
    "sample_twitter_word_embed, twitter_word_dict = encode_words(sample_twitter, column=\"text\")\n",
    "\n",
    "# Example: Accessing Sentence-Level Embeddings\n",
    "print(\"\\nIMDb Sentence Embedding:\", sample_imdb_embed[\"text_embeddings\"][0])\n",
    "\n",
    "# Example: Accessing Word-Level Embeddings\n",
    "print(\"\\nWord Embedding for 'movie' (if present):\", imdb_word_dict.get(\"movie\", \"Not Found\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda8235-6d46-46b0-bb03-1b64fe29687a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Embedding Matrix Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e953302-f122-42e2-b208-df48c6bce6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15757619  0.04378209 -0.00451272  0.06659314  0.07703468  0.00485855\n",
      "  0.00819822  0.00652403  0.009259    0.0353899 ]\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "# Load the FastText model using the absolute path\n",
    "fasttext_model = fasttext.load_model(\"C:/Users/bhall/cc.en.300.bin\")\n",
    "\n",
    "'''# Test: Get word vector for a word\n",
    "word_vector = fasttext_model.get_word_vector(\"hello\")\n",
    "print(word_vector[:10])  # Print first 10 values'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50566dbc-4fe7-4e34-a00b-8081bea2ade4",
   "metadata": {},
   "source": [
    "### Word Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f71f3509-6367-4583-b7a4-78579efff0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(fasttext_model, word_index, embedding_dim=300):\n",
    "    \"\"\"\n",
    "    Creates an embedding matrix using a pre-trained FastText model.\n",
    "\n",
    "    Args:\n",
    "        fasttext_model: Loaded FastText model (`fasttext_model = fasttext.load_model(...)`).\n",
    "        word_index (dict): Tokenizer's word-to-index mapping.\n",
    "        embedding_dim (int): Dimensionality of FastText embeddings (default: 300).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Embedding matrix of shape (vocab_size + 1, embedding_dim).\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, index in word_index.items():\n",
    "        if index >= vocab_size:  # Ignore words exceeding vocab limit\n",
    "            continue\n",
    "\n",
    "        embedding_vector = fasttext_model.get_word_vector(word)\n",
    "        embedding_matrix[index] = embedding_vector  # Assign to matrix\n",
    "\n",
    "    return embedding_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a85d7-7d6b-461f-8ff4-e52f64a52b66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18546c09-da14-4a6e-815c-f255d6ea6553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (4, 300)\n",
      "Example vector for 'hello': [ 0.15757619  0.04378209 -0.00451272  0.06659314  0.07703468  0.00485855\n",
      "  0.00819822  0.00652403  0.009259    0.0353899 ]\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "word_index = {\"hello\": 1, \"world\": 2, \"example\": 3}  # Replace with actual tokenizer.word_index\n",
    "embedding_matrix = create_embedding_matrix(fasttext_model, word_index)\n",
    "\n",
    "print(\"Shape of embedding matrix:\", embedding_matrix.shape)\n",
    "print(\"Example vector for 'hello':\", embedding_matrix[1][:10])  # First 10 values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382b9b4-47ab-4230-9874-3746346fb087",
   "metadata": {},
   "source": [
    "### Sentence Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc78ee25-c700-402a-8d2d-3d3fd5d18f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentences, model=embedding_model, batch_size=32):\n",
    "    \"\"\"\n",
    "    Converts a list of sentences into sentence embeddings.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): List of text sentences.\n",
    "        model: Pre-loaded SentenceTransformer model (default: embedding_model).\n",
    "        batch_size (int): Number of sentences per batch.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Sentence embeddings (num_sentences, embedding_dim).\n",
    "    \"\"\"\n",
    "    return model.encode(sentences, batch_size=batch_size, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee03cc2-cc63-4b53-9cd3-938d3f5d3114",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing the sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b603022-83f3-40bf-bdbf-90e4f203b75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings shape: (2, 384)\n",
      "\n",
      "Example Sentence Embedding: [-0.02230717 -0.06976169  0.03644417 -0.03894086  0.04212659 -0.03013989\n",
      "  0.06765563  0.0234887   0.06278943  0.02705796]\n"
     ]
    }
   ],
   "source": [
    "# Example Usage - Hugging Face\n",
    "sentences = [\"This is a test.\", \"Sentence embeddings are useful.\"]\n",
    "sentence_embeddings = get_sentence_embeddings(sentences)\n",
    "\n",
    "print(\"Sentence embeddings shape:\", sentence_embeddings.shape)\n",
    "\n",
    "# Example Useage - PD Dataset\n",
    "sample_df = pd.DataFrame({\"text\": [\"I love AI.\", \"Transformers are amazing.\", \"Sentence embeddings are powerful.\"]})\n",
    "sample_df = encode_sentences(sample_df, column=\"text\")\n",
    "\n",
    "print(\"\\nExample Sentence Embedding:\", sample_df[\"text_embeddings\"][0][:10])  # Print first 10 values of first embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd488a-3c5d-40da-a472-341bd8b1fb87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch GPU)",
   "language": "python",
   "name": "pytorch_env_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
