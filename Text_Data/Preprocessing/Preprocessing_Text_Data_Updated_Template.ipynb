{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98fb94c2-7062-41c5-8c97-4cb5ed6c67e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Loading the Datasets for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a65f13e9-62ee-4163-a972-2b5db34db20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMDb Example: {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n",
      "\n",
      "AG News Example: {'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n",
      "\n",
      "Twitter Example: {'text': \"@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", 'date': 'Mon Apr 06 22:19:45 PDT 2009', 'user': '_TheSpecialOne_', 'sentiment': 0, 'query': 'NO_QUERY'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load IMDb (Sentiment Analysis)\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Load AG News (News Classification)\n",
    "ag_news_dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# Load Sentiment140 (Twitter Sentiment)\n",
    "twitter_dataset = load_dataset(\"sentiment140\", trust_remote_code=True)\n",
    "\n",
    "# Print Dataset Samples\n",
    "print(\"\\nIMDb Example:\", imdb_dataset[\"train\"][0])\n",
    "print(\"\\nAG News Example:\", ag_news_dataset[\"train\"][0])\n",
    "print(\"\\nTwitter Example:\", twitter_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05051249-9608-4c48-83eb-d851e7b2d930",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c637db-33cb-4b18-b300-445e66db2141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import contractions\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "# --- Cleaning Setup ---\n",
    "nlp_clean = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "nlp_clean.add_pipe(\"sentencizer\")\n",
    "STOPWORDS = nlp_clean.Defaults.stop_words\n",
    "\n",
    "# Define common informal words & slang\n",
    "INFORMAL_WORDS = {\n",
    "    \"gonna\": \"going to\", \"wanna\": \"want to\", \"gotta\": \"got to\",\n",
    "    \"shoulda\": \"should have\", \"coulda\": \"could have\", \"woulda\": \"would have\",\n",
    "    \"lemme\": \"let me\", \"gimme\": \"give me\", \"outta\": \"out of\",\n",
    "    \"dunno\": \"do not know\", \"kinda\": \"kind of\", \"sorta\": \"sort of\",\n",
    "    \"ain't\": \"is not\", \"ya\": \"you\", \"tho\": \"though\", \"til\": \"until\",\n",
    "    \"cuz\": \"because\", \"coz\": \"because\", \"idk\": \"I do not know\",\n",
    "    \"tbh\": \"to be honest\", \"btw\": \"by the way\", \"u\": \"you\", \"ur\": \"your\",\n",
    "    \"r\": \"are\"\n",
    "}\n",
    "\n",
    "def expand_informal(text):\n",
    "    \"\"\"Replaces informal words using our predefined dictionary.\"\"\"\n",
    "    words = text.split()\n",
    "    words = [INFORMAL_WORDS[word] if word in INFORMAL_WORDS else word for word in words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Stopword list\n",
    "STOPWORDS = nlp.Defaults.stop_words\n",
    "\n",
    "def clean_text(text, remove_punctuation=True, remove_stopwords=True, lemmatize=True, sentence_level=False, display=False):\n",
    "    \"\"\"\n",
    "    Cleans raw text by applying several preprocessing steps:\n",
    "    - Lowercasing and trimming\n",
    "    - Removing URLs, emails, and HTML tags\n",
    "    - Expanding contractions and informal words\n",
    "    - Normalizing whitespace and removing non-ASCII characters\n",
    "    - Optionally removing punctuation, stopwords, and lemmatizing\n",
    "    - Optionally performing sentence-level cleaning via spaCy's sentencizer\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Raw text input.\n",
    "        remove_punctuation (bool): Whether to remove punctuation.\n",
    "        remove_stopwords (bool): Whether to remove stopwords.\n",
    "        lemmatize (bool): Whether to lemmatize tokens.\n",
    "        sentence_level (bool): If True, clean text at sentence level.\n",
    "        display (bool): If True, prints brief debug info.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    if text is None or text.strip() == \"\":\n",
    "        if display:\n",
    "            print(\"[DEBUG] Empty text provided; returning an empty string.\")\n",
    "        return \"\"\n",
    "    \n",
    "    original_text = text  # Store original for comparison\n",
    "\n",
    "    # Lowercase and strip text\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Remove URLs, emails, and HTML tags\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Expand informal words\n",
    "    text = expand_informal(text)\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "\n",
    "    # Remove punctuation if enabled\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Tokenize using spaCy\n",
    "    doc = nlp_clean(text)\n",
    "\n",
    "    if sentence_level:\n",
    "        # Process at sentence-level\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            tokens = [\n",
    "                token.lemma_ if lemmatize else token.text\n",
    "                for token in sent\n",
    "                if not (remove_stopwords and token.text in STOPWORDS)\n",
    "            ]\n",
    "            sentences.append(\" \".join(tokens))\n",
    "        processed_text = \" \".join(sentences)\n",
    "    else:\n",
    "        # Process whole text as a single unit\n",
    "        tokens = [\n",
    "            token.lemma_ if lemmatize else token.text\n",
    "            for token in doc\n",
    "            if not (remove_stopwords and token.text in STOPWORDS)\n",
    "        ]\n",
    "        processed_text = \" \".join(tokens)\n",
    "    \n",
    "    # Debug output\n",
    "    if display:\n",
    "        print(f\"[INFO] Original text (first 50 chars): {original_text[:50]}\")\n",
    "        print(f\"[INFO] Cleaned text (first 50 chars): {processed_text[:50]}\")\n",
    "        print(f\"[INFO] Original length: {len(original_text)}; Cleaned length: {len(processed_text)}\")\n",
    "        if not processed_text:\n",
    "            print(\"[WARNING] Cleaning resulted in an empty string.\")\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "def process_text_batch(texts, remove_punctuation=True, remove_stopwords=True, lemmatize=True, sentence_level=False, max_jobs=-1, display=False):\n",
    "    \"\"\"\n",
    "    Applies text cleaning in parallel to a batch of texts.\n",
    "    \n",
    "    Parameters:\n",
    "        texts (list): List of raw text strings.\n",
    "        remove_punctuation, remove_stopwords, lemmatize, sentence_level: Passed to clean_text.\n",
    "        max_jobs (int): Number of parallel jobs (-1 for all cores).\n",
    "        display (bool): If True, passes display flag to clean_text for debug info.\n",
    "\n",
    "    Returns:\n",
    "        list: List of cleaned texts.\n",
    "    \"\"\"\n",
    "    return Parallel(n_jobs=max_jobs)(\n",
    "        delayed(clean_text)(text, remove_punctuation, remove_stopwords, lemmatize, sentence_level, display=display)\n",
    "        for text in texts\n",
    "    )\n",
    "\n",
    "def apply_cleaning(dataset, columns, batch_size=1000, sentence_level=False, max_jobs=1, display=False):\n",
    "    \"\"\"\n",
    "    Applies text cleaning to either a Hugging Face dataset or a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset, DatasetDict, or pd.DataFrame): The dataset to process.\n",
    "        columns (str or list): Column(s) to clean.\n",
    "        batch_size (int): Batch size for processing.\n",
    "        sentence_level (bool or dict): If True, applies sentence-level cleaning for all columns.\n",
    "                                       If a dict, specify per-column settings (e.g., {\"text\": True, \"summary\": False}).\n",
    "        max_jobs (int): Number of parallel jobs (-1 for max cores).\n",
    "        display (bool): If True, prints out info during processing.\n",
    "\n",
    "    Returns:\n",
    "        Dataset, DatasetDict, or DataFrame with cleaned text.\n",
    "    \"\"\"\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]  # Convert to list if a single column is given\n",
    "\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        for column in columns:\n",
    "            if column not in dataset.columns:\n",
    "                raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "        total_rows = len(dataset)\n",
    "        if display:\n",
    "            print(f\"[INFO] Starting cleaning on DataFrame with {total_rows} rows and columns: {columns}\")\n",
    "\n",
    "        # Only show per-batch info if the dataset is small (e.g., fewer than 1000 rows)\n",
    "        show_batch_info = display and total_rows < 1000\n",
    "\n",
    "        cleaned_texts = {col: [] for col in columns}\n",
    "        for i in tqdm(range(0, total_rows, batch_size), desc=\"Cleaning DataFrame\"):\n",
    "            batch = dataset.iloc[i:i+batch_size]\n",
    "            for column in columns:\n",
    "                # Determine the sentence level setting for the current column\n",
    "                col_sentence_level = sentence_level[column] if isinstance(sentence_level, dict) else sentence_level\n",
    "                if show_batch_info:\n",
    "                    print(f\"[INFO] Cleaning batch rows {i} to {i+batch_size} in column '{column}' (sentence_level={col_sentence_level})\")\n",
    "                cleaned_batch = process_text_batch(\n",
    "                    batch[column].tolist(),\n",
    "                    sentence_level=col_sentence_level,\n",
    "                    max_jobs=max_jobs,\n",
    "                    display=display  # passes through to clean_text for internal debug info\n",
    "                )\n",
    "                cleaned_texts[column].extend(cleaned_batch)\n",
    "\n",
    "        for column in columns:\n",
    "            dataset[column] = cleaned_texts[column]\n",
    "        if display:\n",
    "            print(f\"[INFO] DataFrame cleaning complete. Processed {total_rows} rows.\")\n",
    "        return dataset\n",
    "\n",
    "    elif isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        if display:\n",
    "            print(f\"[INFO] Starting cleaning on Hugging Face dataset for columns: {columns}\")\n",
    "        cleaned_dataset = dataset.map(\n",
    "            lambda batch: {\n",
    "                col: [\n",
    "                    clean_text(\n",
    "                        text,\n",
    "                        sentence_level=(sentence_level[col] if isinstance(sentence_level, dict) else sentence_level),\n",
    "                        display=display\n",
    "                    )\n",
    "                    for text in batch[col]\n",
    "                ]\n",
    "                for col in columns\n",
    "            },\n",
    "            batched=True\n",
    "        )\n",
    "        if display:\n",
    "            print(\"[INFO] Hugging Face dataset cleaning complete.\")\n",
    "        return cleaned_dataset\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type. Use a Hugging Face dataset or pandas DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054833c7-d399-4a2f-8d4b-f682ffbbfe4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing the cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09acb13c-9ed0-47d9-8f08-b84bd5721c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1602ecc50d7421da254d9f5643693ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e912175ee2447db9fa4b54fbdad2815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e6433b63de43f69a7bd35f506db68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00308203b44249e797fd724d0dd40d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMDb Cleaned Example: rent curiousyellow video store controversy surround release 1967 hear seize yous custom try enter country fan film consider controversial myselfthe plot center young swedish drama student name lena want learn life particular want focus attention make sort documentary average swede think certain political issue vietnam war race issue united states ask politician ordinary denizen stockholm opinion politic sex drama teacher classmate marry menwhat kill curiousyellow 40 year ago consider pornographic sex nudity scene far shoot like cheaply porno countryman mind find shock reality sex nudity major staple swedish cinema ingmar bergman arguably answer good old boy john ford sex scene filmsi commend filmmaker fact sex show film show artistic purpose shock people money show pornographic theater america curiousyellow good film want study meat potato pun intend swedish cinema film plot\n",
      "\n",
      "AG News Cleaned Example: wall st bears claw black reuters reuters   shortsellers wall street dwindlingband ultracynic see green\n",
      "\n",
      "Twitter Cleaned Example: switchfoot   awww bummer get david carr day d\n",
      "\n",
      "IMDb (Sentence-Level) Cleaned Example: rent curiousyellow video store controversy surround release 1967 hear seize yous custom try enter country fan film consider controversial myselfthe plot center young swedish drama student lena want learn life particular want focus attention sort documentary average swede think certain political issue vietnam war race issue united states ask politician ordinary denizen stockholm opinion politic sex drama teacher classmate marry menwhat kill curiousyellow 40 year ago consider pornographic sex nudity scene far shoot like cheaply porno countryman mind find shock reality sex nudity major staple swedish cinema ingmar bergman arguably answer good old boy john ford sex scene filmsi commend filmmaker fact sex film artistic purpose shock people money pornographic theater america curiousyellow good film want study meat potato pun intend swedish cinema film plot\n"
     ]
    }
   ],
   "source": [
    "# Load Sample Data\n",
    "sample_imdb = imdb_dataset[\"train\"].select(range(5))\n",
    "sample_ag_news = ag_news_dataset[\"train\"].select(range(5))\n",
    "sample_twitter = twitter_dataset[\"train\"].select(range(5))\n",
    "\n",
    "# Apply Text Cleaning (Word-Level)\n",
    "sample_imdb = apply_cleaning(sample_imdb, columns=\"text\")\n",
    "sample_ag_news = apply_cleaning(sample_ag_news, columns=\"text\")\n",
    "sample_twitter = apply_cleaning(sample_twitter, columns=\"text\")\n",
    "\n",
    "# Apply Text Cleaning to Multiple Columns (Sentence-Level Example)\n",
    "# sample_imdb_multi = apply_cleaning(sample_imdb, columns=[\"text\"], sentence_level={\"text\": True})\n",
    "\n",
    "# Print Cleaned Examples\n",
    "print(\"\\nIMDb Cleaned Example:\", sample_imdb[0][\"text\"])\n",
    "print(\"\\nAG News Cleaned Example:\", sample_ag_news[0][\"text\"])\n",
    "print(\"\\nTwitter Cleaned Example:\", sample_twitter[0][\"text\"])\n",
    "# print(\"\\nIMDb (Sentence-Level) Cleaned Example:\", sample_imdb_multi[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ec9c9-5452-40d3-b6d4-d335f9f85541",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29bfddd8-e422-4e2b-925f-6dd2d943f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp_token = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def basic_tokenize(text):\n",
    "    \"\"\"Basic whitespace-based tokenizer as a fallback.\"\"\"\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())  # Simple word tokenization\n",
    "\n",
    "def tokenize_text(dataset, column=\"text\", model=None, max_length=128, sentence_level=False, display=False):\n",
    "    \"\"\"\n",
    "    Tokenizes text using either a Hugging Face pre-trained model or a simple tokenizer.\n",
    "    Now includes padding & truncation.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset, DatasetDict, or pd.DataFrame): Dataset to tokenize.\n",
    "        column (str): The text column to process.\n",
    "        model (str or None): Pre-trained tokenizer model (e.g., 'bert-base-uncased'). \n",
    "                             If None, uses a basic tokenizer.\n",
    "        max_length (int): Maximum sequence length for padding/truncation (default: 128).\n",
    "        sentence_level (bool): If True, returns sentence segmentation using spaCy.\n",
    "        display (bool): If True, prints a final status message after tokenization.\n",
    "\n",
    "    Returns:\n",
    "        Dataset, DatasetDict, or DataFrame with tokenized output.\n",
    "    \"\"\"\n",
    "    if model:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)  # Load Hugging Face tokenizer\n",
    "    else:\n",
    "        tokenizer = None  # Use basic tokenization instead\n",
    "\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        if column not in dataset.columns:\n",
    "            raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "        def process_text(text):\n",
    "            if tokenizer:\n",
    "                # Tokenize using HF tokenizer and extract input_ids\n",
    "                tokenized_output = tokenizer(\n",
    "                    text,\n",
    "                    padding=\"max_length\",  # Pads to max_length\n",
    "                    truncation=True,       # Truncates long sequences\n",
    "                    max_length=max_length, \n",
    "                )\n",
    "                # Basic check: ensure token IDs are returned\n",
    "                token_ids = tokenized_output.get(\"input_ids\", [])\n",
    "                if not token_ids:\n",
    "                    print(f\"[WARNING] Tokenization produced empty token ids for text: {text[:30]}...\")\n",
    "                return token_ids\n",
    "            else:\n",
    "                return basic_tokenize(text)  # Fallback basic tokenization\n",
    "\n",
    "        dataset[column + \"_token_ids\"] = dataset[column].apply(process_text)\n",
    "        if display:\n",
    "            print(f\"[INFO] Tokenization completed on DataFrame: {len(dataset)} rows processed.\")\n",
    "        return dataset\n",
    "\n",
    "    elif isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        def process_batch(batch):\n",
    "            \"\"\"Handles batch processing to avoid type errors.\"\"\"\n",
    "            texts = batch[column]\n",
    "            if tokenizer:\n",
    "                tokenized_outputs = tokenizer(\n",
    "                    texts,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    return_tensors=None  # Return Python lists\n",
    "                )\n",
    "                token_ids = tokenized_outputs.get(\"input_ids\", [])\n",
    "                tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in token_ids]\n",
    "            else:\n",
    "                tokens = [basic_tokenize(text) for text in texts]\n",
    "                token_ids = None  # Not used for basic tokenization\n",
    "\n",
    "            # Process sentence segmentation if requested\n",
    "            sentences = []\n",
    "            if sentence_level:\n",
    "                for text in texts:\n",
    "                    sents = [sent.text for sent in nlp(text).sents]\n",
    "                    sentences.append(sents)\n",
    "            else:\n",
    "                sentences = [None] * len(texts)\n",
    "\n",
    "            return {\n",
    "                column + \"_tokens\": tokens,\n",
    "                column + \"_token_ids\": token_ids if token_ids else [None] * len(tokens),\n",
    "                column + \"_sentences\": sentences\n",
    "            }\n",
    "\n",
    "        tokenized_dataset = dataset.map(process_batch, batched=True)\n",
    "        if display:\n",
    "            print(f\"[INFO] Tokenization completed on Hugging Face dataset for column '{column}'.\")\n",
    "        return tokenized_dataset\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type. Use a Hugging Face dataset or pandas DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d5486-f08a-42a5-9265-9feb94b574c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96aba67b-4d47-42a0-a70b-b7a0092e806b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6d99d73aeb4caea86a14a3545842e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007f9e5951bd4b2c853a5656f616cbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15816b61a714414c9226813c5e165d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4141b953aea74b43adc84b82de46a54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IMDb Tokenized Example BERT: ['[CLS]', 'rent', 'curious', '##ye', '##llo', '##w', 'video', 'store', 'controversy', 'surround', 'release', '1967', 'hear', 'seize', 'you', '##s', 'custom', 'try', 'enter', 'country', 'fan', 'film', 'consider', 'controversial', 'myself', '##the', 'plot', 'center', 'young', 'swedish', 'drama', 'student', 'name', 'lena', 'want', 'learn', 'life', 'particular', 'want', 'focus', 'attention', 'make', 'sort', 'documentary', 'average', 'sw', '##ede', 'think', 'certain', 'political', 'issue', 'vietnam', 'war', 'race', 'issue', 'united', 'states', 'ask', 'politician', 'ordinary', 'den', '##ize', '##n', 'stockholm', 'opinion', 'pol', '##itic', 'sex', 'drama', 'teacher', 'classmate', 'marry', 'men', '##w', '##hat', 'kill', 'curious', '##ye', '##llo', '##w', '40', 'year', 'ago', 'consider', 'pornographic', 'sex', 'nu', '##dity', 'scene', 'far', 'shoot', 'like', 'cheap', '##ly', 'porn', '##o', 'country', '##man', 'mind', 'find', 'shock', 'reality', 'sex', 'nu', '##dity', 'major', 'staple', 'swedish', 'cinema', 'ing', '##mar', 'bergman', 'arguably', 'answer', 'good', 'old', 'boy', 'john', 'ford', 'sex', 'scene', 'films', '##i', 'com', '##men', '##d', 'filmmaker', '[SEP]']\n",
      "\n",
      "IMDb Tokenized Example Simple: ['rent', 'curiousyellow', 'video', 'store', 'controversy', 'surround', 'release', '1967', 'hear', 'seize', 'yous', 'custom', 'try', 'enter', 'country', 'fan', 'film', 'consider', 'controversial', 'myselfthe', 'plot', 'center', 'young', 'swedish', 'drama', 'student', 'name', 'lena', 'want', 'learn', 'life', 'particular', 'want', 'focus', 'attention', 'make', 'sort', 'documentary', 'average', 'swede', 'think', 'certain', 'political', 'issue', 'vietnam', 'war', 'race', 'issue', 'united', 'states', 'ask', 'politician', 'ordinary', 'denizen', 'stockholm', 'opinion', 'politic', 'sex', 'drama', 'teacher', 'classmate', 'marry', 'menwhat', 'kill', 'curiousyellow', '40', 'year', 'ago', 'consider', 'pornographic', 'sex', 'nudity', 'scene', 'far', 'shoot', 'like', 'cheaply', 'porno', 'countryman', 'mind', 'find', 'shock', 'reality', 'sex', 'nudity', 'major', 'staple', 'swedish', 'cinema', 'ingmar', 'bergman', 'arguably', 'answer', 'good', 'old', 'boy', 'john', 'ford', 'sex', 'scene', 'filmsi', 'commend', 'filmmaker', 'fact', 'sex', 'show', 'film', 'show', 'artistic', 'purpose', 'shock', 'people', 'money', 'show', 'pornographic', 'theater', 'america', 'curiousyellow', 'good', 'film', 'want', 'study', 'meat', 'potato', 'pun', 'intend', 'swedish', 'cinema', 'film', 'plot']\n",
      "\n",
      "AG News Tokenized Example: ['wall', 'st', 'bears', 'claw', 'black', 'reuters', 'reuters', 'shortsellers', 'wall', 'street', 'dwindlingband', 'ultracynic', 'see', 'green']\n",
      "\n",
      "Twitter (Sentence-Level) Example: ['switchfoot   awww bummer get david carr day d']\n"
     ]
    }
   ],
   "source": [
    "# Apply tokenization with the updated function\n",
    "sample_imdb_tokenized_model = tokenize_text(sample_imdb, column=\"text\", model=\"bert-base-uncased\")\n",
    "sample_imdb_tokenized_simple = tokenize_text(sample_imdb, column=\"text\", model=None)\n",
    "sample_ag_news_tokenized = tokenize_text(sample_ag_news, column=\"text\")\n",
    "sample_twitter_tokenized = tokenize_text(sample_twitter, column=\"text\", sentence_level=True)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nIMDb Tokenized Example BERT:\", sample_imdb_tokenized_model[0][\"text_tokens\"])\n",
    "print(\"\\nIMDb Tokenized Example Simple:\", sample_imdb_tokenized_simple[0][\"text_tokens\"])\n",
    "print(\"\\nAG News Tokenized Example:\", sample_ag_news_tokenized[0][\"text_tokens\"])\n",
    "print(\"\\nTwitter (Sentence-Level) Example:\", sample_twitter_tokenized[0][\"text_sentences\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a91c0-91ff-4873-873b-ee0a5744e829",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f4f664-c5ec-4242-9ffa-44f20a80fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Load SentenceTransformer Model (small but powerful)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load spaCy model with vectors (make sure you have en_core_web_md installed)\n",
    "nlp_vec = spacy.load(\"en_core_web_md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51525fc8-6d6b-4b7b-8bfb-886680a7e156",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd567e9-83f9-4d99-9442-169b8d5bd2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(dataset, column=\"text\", batch_size=32, display=False):\n",
    "    \"\"\"\n",
    "    Encodes words individually into embeddings using spaCy's word vectors.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset, DatasetDict, or pd.DataFrame): The dataset to process.\n",
    "        column (str): The column containing text.\n",
    "        batch_size (int): Placeholder for compatibility (unused here).\n",
    "        display (bool): If True, prints a final summary message after encoding.\n",
    "\n",
    "    Returns:\n",
    "        (processed_dataset, word_embeddings_dict)\n",
    "            processed_dataset: The dataset with an added column (e.g. \"text_word_embeddings\") containing lists of embeddings.\n",
    "            word_embeddings_dict: A dictionary mapping each encountered word to its embedding.\n",
    "    \"\"\"\n",
    "    word_embeddings_dict = {}\n",
    "\n",
    "    def get_word_embeddings(text):\n",
    "        doc = nlp_vec(text)\n",
    "        embeddings = []\n",
    "        for token in doc:\n",
    "            word = token.text\n",
    "            emb = token.vector  # Get spaCy's vector for this token\n",
    "            embeddings.append(emb)\n",
    "            word_embeddings_dict[word] = emb  # Update dictionary (duplicates get overwritten with identical vectors)\n",
    "        return embeddings\n",
    "\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        # Apply function to every row in the DataFrame column\n",
    "        dataset[column + \"_word_embeddings\"] = dataset[column].apply(get_word_embeddings)\n",
    "        if display:\n",
    "            print(f\"[INFO] DataFrame processing complete. Processed {len(dataset)} rows.\")\n",
    "            print(f\"[INFO] Vocabulary size (unique words embedded): {len(word_embeddings_dict)}\")\n",
    "        return dataset, word_embeddings_dict\n",
    "\n",
    "    elif isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        def process_batch(batch):\n",
    "            embeddings_batch = []\n",
    "            for text in batch[column]:\n",
    "                word_embeds = get_word_embeddings(text)\n",
    "                embeddings_batch.append(word_embeds)\n",
    "            return {column + \"_word_embeddings\": embeddings_batch}\n",
    "\n",
    "        processed_dataset = dataset.map(process_batch, batched=True)\n",
    "        if display:\n",
    "            total = processed_dataset.num_rows if hasattr(processed_dataset, \"num_rows\") else \"unknown\"\n",
    "            print(f\"[INFO] Hugging Face dataset processing complete. Processed {total} rows.\")\n",
    "            print(f\"[INFO] Vocabulary size (unique words embedded): {len(word_embeddings_dict)}\")\n",
    "        return processed_dataset, word_embeddings_dict\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type. Use a Hugging Face dataset or pandas DataFrame.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88adef-553d-4801-974f-5094e44e209d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b32ef91-21ac-4f5c-8cfd-7c59e312705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(dataset, column=\"text\", model=embedding_model, batch_size=32, num_proc=None):\n",
    "    \"\"\"\n",
    "    Encodes sentences into embeddings using SentenceTransformers.\n",
    "    Now supports parallel processing for Hugging Face datasets.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset, DatasetDict, or DataFrame): The dataset to process.\n",
    "        column (str): The column containing text.\n",
    "        model: Pre-loaded SentenceTransformer model (default: embedding_model).\n",
    "        batch_size (int): Number of sentences per batch.\n",
    "        num_proc (int, optional): Number of processes to use for multiprocessing (Only for Hugging Face datasets).\n",
    "                                  Set `num_proc=-1` to use all available CPU cores.\n",
    "\n",
    "    Returns:\n",
    "        Dataset, DatasetDict, or DataFrame with embeddings stored.\n",
    "    \"\"\"\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        # Convert column to list\n",
    "        sentences = dataset[column].tolist()\n",
    "        # Generate embeddings in batches using the provided model\n",
    "        embeddings = model.encode(sentences, batch_size=batch_size, convert_to_numpy=True)\n",
    "        # Store embeddings\n",
    "        dataset[column + \"_embeddings\"] = list(embeddings)\n",
    "        return dataset\n",
    "\n",
    "    elif isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        # Parallel Processing enabled for Hugging Face Datasets\n",
    "        num_proc = num_proc if num_proc else 1  # Default to 1 process if not set\n",
    "\n",
    "        return dataset.map(\n",
    "            lambda x: {column + \"_embeddings\": model.encode(x[column], batch_size=batch_size, convert_to_numpy=True)},\n",
    "            batched=True,\n",
    "            num_proc=num_proc  # This enables multiprocessing\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type. Use a Hugging Face dataset or pandas DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7306af32-3cc1-4258-a245-37c7f23caf20",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c315ede-2bd6-4e46-871a-fb9383d6d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentence-level embeddings\n",
    "sample_imdb_embed = encode_sentences(sample_imdb, column=\"text\")\n",
    "sample_ag_news_embed = encode_sentences(sample_ag_news, column=\"text\")\n",
    "sample_twitter_embed = encode_sentences(sample_twitter, column=\"text\")\n",
    "\n",
    "# Apply word-level embeddings\n",
    "sample_imdb_word_embed, imdb_word_dict = encode_words(sample_imdb, column=\"text\")\n",
    "sample_ag_news_word_embed, ag_news_word_dict = encode_words(sample_ag_news, column=\"text\")\n",
    "sample_twitter_word_embed, twitter_word_dict = encode_words(sample_twitter, column=\"text\")\n",
    "\n",
    "# Example: Accessing Sentence-Level Embeddings\n",
    "print(\"\\nIMDb Sentence Embedding:\", sample_imdb_embed[\"text_embeddings\"][0])\n",
    "\n",
    "# Example: Accessing Word-Level Embeddings\n",
    "print(\"\\nWord Embedding for 'movie' (if present):\", imdb_word_dict.get(\"movie\", \"Not Found\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda8235-6d46-46b0-bb03-1b64fe29687a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Embedding Matrix Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e953302-f122-42e2-b208-df48c6bce6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15757619  0.04378209 -0.00451272  0.06659314  0.07703468  0.00485855\n",
      "  0.00819822  0.00652403  0.009259    0.0353899 ]\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "# Load the FastText model using the absolute path\n",
    "fasttext_model = fasttext.load_model(\"C:/Users/bhall/cc.en.300.bin\")\n",
    "\n",
    "'''# Test: Get word vector for a word\n",
    "word_vector = fasttext_model.get_word_vector(\"hello\")\n",
    "print(word_vector[:10])  # Print first 10 values'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50566dbc-4fe7-4e34-a00b-8081bea2ade4",
   "metadata": {},
   "source": [
    "### Word Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f71f3509-6367-4583-b7a4-78579efff0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(fasttext_model, word_index, embedding_dim=300, display=False):\n",
    "    \"\"\"\n",
    "    Creates an embedding matrix using a pre-trained FastText model.\n",
    "\n",
    "    Args:\n",
    "        fasttext_model: Loaded FastText model.\n",
    "        word_index (dict): Tokenizer's word-to-index mapping.\n",
    "        embedding_dim (int): Dimensionality of FastText embeddings (default: 300).\n",
    "        display (bool): If True, prints a final summary message.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Embedding matrix of shape (vocab_size, embedding_dim).\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, index in word_index.items():\n",
    "        if index >= vocab_size:  # Ignore words exceeding vocab limit\n",
    "            continue\n",
    "        embedding_vector = fasttext_model.get_word_vector(word)\n",
    "        embedding_matrix[index] = embedding_vector  # Assign to matrix\n",
    "\n",
    "    if display:\n",
    "        print(f\"[INFO] Created embedding matrix with shape: {embedding_matrix.shape}\")\n",
    "\n",
    "    return embedding_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a85d7-7d6b-461f-8ff4-e52f64a52b66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18546c09-da14-4a6e-815c-f255d6ea6553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embedding matrix: (4, 300)\n",
      "Example vector for 'hello': [ 0.15757619  0.04378209 -0.00451272  0.06659314  0.07703468  0.00485855\n",
      "  0.00819822  0.00652403  0.009259    0.0353899 ]\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "word_index = {\"hello\": 1, \"world\": 2, \"example\": 3}  # Replace with actual tokenizer.word_index\n",
    "embedding_matrix = create_embedding_matrix(fasttext_model, word_index)\n",
    "\n",
    "print(\"Shape of embedding matrix:\", embedding_matrix.shape)\n",
    "print(\"Example vector for 'hello':\", embedding_matrix[1][:10])  # First 10 values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382b9b4-47ab-4230-9874-3746346fb087",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sentence Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc78ee25-c700-402a-8d2d-3d3fd5d18f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentences, model=embedding_model, batch_size=32):\n",
    "    \"\"\"\n",
    "    Converts a list of sentences into sentence embeddings.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): List of text sentences.\n",
    "        model: Pre-loaded SentenceTransformer model (default: embedding_model).\n",
    "        batch_size (int): Number of sentences per batch.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Sentence embeddings (num_sentences, embedding_dim).\n",
    "    \"\"\"\n",
    "    return model.encode(sentences, batch_size=batch_size, convert_to_numpy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee03cc2-cc63-4b53-9cd3-938d3f5d3114",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing the sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b603022-83f3-40bf-bdbf-90e4f203b75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings shape: (2, 384)\n",
      "\n",
      "Example Sentence Embedding: [-0.02230717 -0.06976169  0.03644417 -0.03894086  0.04212659 -0.03013989\n",
      "  0.06765563  0.0234887   0.06278943  0.02705796]\n"
     ]
    }
   ],
   "source": [
    "# Example Usage - Hugging Face\n",
    "sentences = [\"This is a test.\", \"Sentence embeddings are useful.\"]\n",
    "sentence_embeddings = get_sentence_embeddings(sentences)\n",
    "\n",
    "print(\"Sentence embeddings shape:\", sentence_embeddings.shape)\n",
    "\n",
    "# Example Useage - PD Dataset\n",
    "sample_df = pd.DataFrame({\"text\": [\"I love AI.\", \"Transformers are amazing.\", \"Sentence embeddings are powerful.\"]})\n",
    "sample_df = encode_sentences(sample_df, column=\"text\")\n",
    "\n",
    "print(\"\\nExample Sentence Embedding:\", sample_df[\"text_embeddings\"][0][:10])  # Print first 10 values of first embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ae164a-aa98-4c99-876d-8b3e2f546ffc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Converting data to tensors and prepping for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953c1a3-bc7f-4ae9-b7d5-b6ca57da11d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def convert_to_tensors(dataset, columns, labels_column=None, display=False):\n",
    "    \"\"\"\n",
    "    Converts specified dataset columns to PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset, DatasetDict, or pd.DataFrame): The dataset to process.\n",
    "        columns (list): List of columns to convert to tensors.\n",
    "        labels_column (str, optional): Column containing labels (for supervised tasks).\n",
    "        display (bool, optional): If True, prints out summary info after conversion.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing tensors for each specified column.\n",
    "    \"\"\"\n",
    "    tensor_dict = {}\n",
    "\n",
    "    for col in columns:\n",
    "        data = dataset[col]\n",
    "\n",
    "        # If data is a pandas Series, force each element to be a list\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = data.apply(lambda x: list(x) if not isinstance(x, list) else x)\n",
    "            data = list(data)  # Convert Series into a list\n",
    "\n",
    "        if isinstance(data, (list, np.ndarray)):\n",
    "            if col.endswith(\"_token_ids\"):\n",
    "                data = torch.tensor(data, dtype=torch.long)\n",
    "            else:\n",
    "                data = torch.tensor(data, dtype=torch.float32)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data type for column: {col}\")\n",
    "\n",
    "        tensor_dict[col] = data\n",
    "\n",
    "    if labels_column:\n",
    "        labels = torch.tensor(dataset[labels_column].values, dtype=torch.long)\n",
    "        tensor_dict[\"labels\"] = labels\n",
    "\n",
    "    if display:\n",
    "        print(\"[INFO] Conversion to tensors complete. Summary:\")\n",
    "        for key, tensor in tensor_dict.items():\n",
    "            print(f\"  {key}: shape = {tensor.shape}\")\n",
    "\n",
    "    return tensor_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f3a70-1ba5-446e-859a-2aefd982c77d",
   "metadata": {},
   "source": [
    "### Dataset/DataLoader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed8be797-952c-4782-956b-f580f7bcffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, tensor_dict):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset for text-based data.\n",
    "\n",
    "        Args:\n",
    "            tensor_dict (dict): Dictionary containing tensors (including optional labels).\n",
    "        \"\"\"\n",
    "        self.data = tensor_dict\n",
    "        self.keys = list(tensor_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        # Assumes all tensor entries have the same length\n",
    "        return len(self.data[self.keys[0]])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {key: self.data[key][idx] for key in self.keys}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6292a928-5bd1-4249-a02b-a2ca7f31a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloader(tensor_dict, batch_size=32, shuffle=True, display=False):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for batch processing.\n",
    "\n",
    "    Args:\n",
    "        tensor_dict (dict): Dictionary of tensors (from convert_to_tensors).\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        shuffle (bool): Whether to shuffle data.\n",
    "        display (bool, optional): If True, prints out summary info after creation.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader object.\n",
    "    \"\"\"\n",
    "    dataset = CustomTextDataset(tensor_dict)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    if display:\n",
    "        num_samples = len(dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        print(f\"[INFO] DataLoader created: {num_samples} samples, batch size = {batch_size}, total batches = {num_batches}\")\n",
    "\n",
    "    return dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7000e8-c0ed-41dd-b1c8-0f871ff41837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Assume `processed_dataset` is our cleaned + tokenized dataset\n",
    "columns_to_convert = [\"text_token_ids\", \"text_embeddings\"]  # Example columns\n",
    "tensor_dict = convert_to_tensors(processed_dataset, columns_to_convert, labels_column=\"label\")\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = create_dataloader(tensor_dict, batch_size=32)\n",
    "\n",
    "# Example: Iterate through DataLoader\n",
    "for batch in dataloader:\n",
    "    print(batch[\"text_token_ids\"].shape, batch[\"text_embeddings\"].shape, batch[\"labels\"].shape)\n",
    "    break  # Print one batch for checking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85840c-c55d-43f6-8baf-2acbb50d1e5a",
   "metadata": {},
   "source": [
    "## Copy Paste Template - Contains all the templates (word embedding version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "627c2f27-8e85-4e3e-9b15-e3c369afabfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 221\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetDict\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env_new\\lib\\site-packages\\transformers\\__init__.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[0;32m     29\u001b[0m     _LazyModule,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     logging,\n\u001b[0;32m     49\u001b[0m )\n\u001b[0;32m     52\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env_new\\lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[0;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     38\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env_new\\lib\\site-packages\\transformers\\utils\\__init__.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     add_code_sample_docstrings,\n\u001b[0;32m     31\u001b[0m     add_end_docstrings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     replace_return_docstrings,\n\u001b[0;32m     36\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env_new\\lib\\site-packages\\transformers\\utils\\chat_template_utils.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m     jinja2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_vision_available():\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mImage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env_new\\lib\\site-packages\\PIL\\Image.py:97\u001b[0m\n\u001b[0;32m     88\u001b[0m MAX_IMAGE_PIXELS: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1024\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;66;03m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _imaging \u001b[38;5;28;01mas\u001b[39;00m core\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __version__ \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(core, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    100\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCore version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(core,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPILLOW_VERSION\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPillow version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import contractions\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "# --- Cleaning Setup ---\n",
    "nlp_clean = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "nlp_clean.add_pipe(\"sentencizer\")\n",
    "STOPWORDS = nlp_clean.Defaults.stop_words\n",
    "\n",
    "# Define common informal words & slang\n",
    "INFORMAL_WORDS = {\n",
    "    \"gonna\": \"going to\", \"wanna\": \"want to\", \"gotta\": \"got to\",\n",
    "    \"shoulda\": \"should have\", \"coulda\": \"could have\", \"woulda\": \"would have\",\n",
    "    \"lemme\": \"let me\", \"gimme\": \"give me\", \"outta\": \"out of\",\n",
    "    \"dunno\": \"do not know\", \"kinda\": \"kind of\", \"sorta\": \"sort of\",\n",
    "    \"ain't\": \"is not\", \"ya\": \"you\", \"tho\": \"though\", \"til\": \"until\",\n",
    "    \"cuz\": \"because\", \"coz\": \"because\", \"idk\": \"I do not know\",\n",
    "    \"tbh\": \"to be honest\", \"btw\": \"by the way\", \"u\": \"you\", \"ur\": \"your\",\n",
    "    \"r\": \"are\"\n",
    "}\n",
    "\n",
    "def expand_informal(text):\n",
    "    \"\"\"Replaces informal words using our predefined dictionary.\"\"\"\n",
    "    words = text.split()\n",
    "    words = [INFORMAL_WORDS[word] if word in INFORMAL_WORDS else word for word in words]\n",
    "    return \" \".join(words)\n",
    "    \n",
    "\n",
    "def clean_text(text, remove_punctuation=True, remove_stopwords=True, lemmatize=True, sentence_level=False, display=False):\n",
    "    \"\"\"\n",
    "    Cleans raw text by applying several preprocessing steps:\n",
    "    - Lowercasing and trimming\n",
    "    - Removing URLs, emails, and HTML tags\n",
    "    - Expanding contractions and informal words\n",
    "    - Normalizing whitespace and removing non-ASCII characters\n",
    "    - Optionally removing punctuation, stopwords, and lemmatizing\n",
    "    - Optionally performing sentence-level cleaning via spaCy's sentencizer\n",
    "\n",
    "    Parameters:\n",
    "        text (str): Raw text input.\n",
    "        remove_punctuation (bool): Whether to remove punctuation.\n",
    "        remove_stopwords (bool): Whether to remove stopwords.\n",
    "        lemmatize (bool): Whether to lemmatize tokens.\n",
    "        sentence_level (bool): If True, clean text at sentence level.\n",
    "        display (bool): If True, prints brief debug info.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    if text is None or text.strip() == \"\":\n",
    "        if display:\n",
    "            print(\"[DEBUG] Empty text provided; returning an empty string.\")\n",
    "        return \"\"\n",
    "    \n",
    "    original_text = text  # Store original for comparison\n",
    "\n",
    "    # Lowercase and strip text\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Remove URLs, emails, and HTML tags\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "    text = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Expand informal words\n",
    "    text = expand_informal(text)\n",
    "\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "\n",
    "    # Remove punctuation if enabled\n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Tokenize using spaCy\n",
    "    doc = nlp_clean(text)\n",
    "\n",
    "    if sentence_level:\n",
    "        # Process at sentence-level\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            tokens = [\n",
    "                token.lemma_ if lemmatize else token.text\n",
    "                for token in sent\n",
    "                if not (remove_stopwords and token.text in STOPWORDS)\n",
    "            ]\n",
    "            sentences.append(\" \".join(tokens))\n",
    "        processed_text = \" \".join(sentences)\n",
    "    else:\n",
    "        # Process whole text as a single unit\n",
    "        tokens = [\n",
    "            token.lemma_ if lemmatize else token.text\n",
    "            for token in doc\n",
    "            if not (remove_stopwords and token.text in STOPWORDS)\n",
    "        ]\n",
    "        processed_text = \" \".join(tokens)\n",
    "    \n",
    "    # Debug output\n",
    "    if display:\n",
    "        print(f\"[INFO] Original text (first 50 chars): {original_text[:50]}\")\n",
    "        print(f\"[INFO] Cleaned text (first 50 chars): {processed_text[:50]}\")\n",
    "        print(f\"[INFO] Original length: {len(original_text)}; Cleaned length: {len(processed_text)}\")\n",
    "        if not processed_text:\n",
    "            print(\"[WARNING] Cleaning resulted in an empty string.\")\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "def process_text_batch(texts, remove_punctuation=True, remove_stopwords=True, lemmatize=True, sentence_level=False, max_jobs=-1, display=False):\n",
    "    \"\"\"\n",
    "    Applies text cleaning in parallel to a batch of texts.\n",
    "    \n",
    "    Parameters:\n",
    "        texts (list): List of raw text strings.\n",
    "        remove_punctuation, remove_stopwords, lemmatize, sentence_level: Passed to clean_text.\n",
    "        max_jobs (int): Number of parallel jobs (-1 for all cores).\n",
    "        display (bool): If True, passes display flag to clean_text for debug info.\n",
    "\n",
    "    Returns:\n",
    "        list: List of cleaned texts.\n",
    "    \"\"\"\n",
    "    return Parallel(n_jobs=max_jobs)(\n",
    "        delayed(clean_text)(text, remove_punctuation, remove_stopwords, lemmatize, sentence_level, display=display)\n",
    "        for text in texts\n",
    "    )\n",
    "\n",
    "def apply_cleaning(dataset, columns, batch_size=1000, sentence_level=False, max_jobs=1, display=False):\n",
    "    \"\"\"\n",
    "    Applies text cleaning to either a Hugging Face dataset or a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset, DatasetDict, or pd.DataFrame): The dataset to process.\n",
    "        columns (str or list): Column(s) to clean.\n",
    "        batch_size (int): Batch size for processing.\n",
    "        sentence_level (bool or dict): If True, applies sentence-level cleaning for all columns.\n",
    "                                       If a dict, specify per-column settings (e.g., {\"text\": True, \"summary\": False}).\n",
    "        max_jobs (int): Number of parallel jobs (-1 for max cores).\n",
    "        display (bool): If True, prints out info during processing.\n",
    "\n",
    "    Returns:\n",
    "        Dataset, DatasetDict, or DataFrame with cleaned text.\n",
    "    \"\"\"\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]  # Convert to list if a single column is given\n",
    "\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        for column in columns:\n",
    "            if column not in dataset.columns:\n",
    "                raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "        total_rows = len(dataset)\n",
    "        if display:\n",
    "            print(f\"[INFO] Starting cleaning on DataFrame with {total_rows} rows and columns: {columns}\")\n",
    "\n",
    "        # Only show per-batch info if the dataset is small (e.g., fewer than 1000 rows)\n",
    "        show_batch_info = display and total_rows < 1000\n",
    "\n",
    "        cleaned_texts = {col: [] for col in columns}\n",
    "        for i in tqdm(range(0, total_rows, batch_size), desc=\"Cleaning DataFrame\"):\n",
    "            batch = dataset.iloc[i:i+batch_size]\n",
    "            for column in columns:\n",
    "                # Determine the sentence level setting for the current column\n",
    "                col_sentence_level = sentence_level[column] if isinstance(sentence_level, dict) else sentence_level\n",
    "                if show_batch_info:\n",
    "                    print(f\"[INFO] Cleaning batch rows {i} to {i+batch_size} in column '{column}' (sentence_level={col_sentence_level})\")\n",
    "                cleaned_batch = process_text_batch(\n",
    "                    batch[column].tolist(),\n",
    "                    sentence_level=col_sentence_level,\n",
    "                    max_jobs=max_jobs,\n",
    "                    display=display  # passes through to clean_text for internal debug info\n",
    "                )\n",
    "                cleaned_texts[column].extend(cleaned_batch)\n",
    "\n",
    "        for column in columns:\n",
    "            dataset[column] = cleaned_texts[column]\n",
    "        if display:\n",
    "            print(f\"[INFO] DataFrame cleaning complete. Processed {total_rows} rows.\")\n",
    "        return dataset\n",
    "\n",
    "    elif isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        if display:\n",
    "            print(f\"[INFO] Starting cleaning on Hugging Face dataset for columns: {columns}\")\n",
    "        cleaned_dataset = dataset.map(\n",
    "            lambda batch: {\n",
    "                col: [\n",
    "                    clean_text(\n",
    "                        text,\n",
    "                        sentence_level=(sentence_level[col] if isinstance(sentence_level, dict) else sentence_level),\n",
    "                        display=display\n",
    "                    )\n",
    "                    for text in batch[col]\n",
    "                ]\n",
    "                for col in columns\n",
    "            },\n",
    "            batched=True\n",
    "        )\n",
    "        if display:\n",
    "            print(\"[INFO] Hugging Face dataset cleaning complete.\")\n",
    "        return cleaned_dataset\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type. Use a Hugging Face dataset or pandas DataFrame.\")\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp_token = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def basic_tokenize(text):\n",
    "    \"\"\"Basic whitespace-based tokenizer as a fallback.\"\"\"\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())  # Simple word tokenization\n",
    "\n",
    "def tokenize_text(dataset, column=\"text\", model=None, max_length=128, sentence_level=False, display=False):\n",
    "    \"\"\"\n",
    "    Tokenizes text using either a Hugging Face pre-trained model or a simple tokenizer.\n",
    "    Now includes padding & truncation.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset, DatasetDict, or pd.DataFrame): Dataset to tokenize.\n",
    "        column (str): The text column to process.\n",
    "        model (str or None): Pre-trained tokenizer model (e.g., 'bert-base-uncased'). \n",
    "                             If None, uses a basic tokenizer.\n",
    "        max_length (int): Maximum sequence length for padding/truncation (default: 128).\n",
    "        sentence_level (bool): If True, returns sentence segmentation using spaCy.\n",
    "        display (bool): If True, prints a final status message after tokenization.\n",
    "\n",
    "    Returns:\n",
    "        Dataset, DatasetDict, or DataFrame with tokenized output.\n",
    "    \"\"\"\n",
    "    if model:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model)  # Load Hugging Face tokenizer\n",
    "    else:\n",
    "        tokenizer = None  # Use basic tokenization instead\n",
    "\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        if column not in dataset.columns:\n",
    "            raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "        def process_text(text):\n",
    "            if tokenizer:\n",
    "                # Tokenize using HF tokenizer and extract input_ids\n",
    "                tokenized_output = tokenizer(\n",
    "                    text,\n",
    "                    padding=\"max_length\",  # Pads to max_length\n",
    "                    truncation=True,       # Truncates long sequences\n",
    "                    max_length=max_length, \n",
    "                )\n",
    "                # Basic check: ensure token IDs are returned\n",
    "                token_ids = tokenized_output.get(\"input_ids\", [])\n",
    "                if not token_ids:\n",
    "                    print(f\"[WARNING] Tokenization produced empty token ids for text: {text[:30]}...\")\n",
    "                return token_ids\n",
    "            else:\n",
    "                return basic_tokenize(text)  # Fallback basic tokenization\n",
    "\n",
    "        dataset[column + \"_token_ids\"] = dataset[column].apply(process_text)\n",
    "        if display:\n",
    "            print(f\"[INFO] Tokenization completed on DataFrame: {len(dataset)} rows processed.\")\n",
    "        return dataset\n",
    "\n",
    "    elif isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        def process_batch(batch):\n",
    "            \"\"\"Handles batch processing to avoid type errors.\"\"\"\n",
    "            texts = batch[column]\n",
    "            if tokenizer:\n",
    "                tokenized_outputs = tokenizer(\n",
    "                    texts,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    return_tensors=None  # Return Python lists\n",
    "                )\n",
    "                token_ids = tokenized_outputs.get(\"input_ids\", [])\n",
    "                tokens = [tokenizer.convert_ids_to_tokens(ids) for ids in token_ids]\n",
    "            else:\n",
    "                tokens = [basic_tokenize(text) for text in texts]\n",
    "                token_ids = None  # Not used for basic tokenization\n",
    "\n",
    "            # Process sentence segmentation if requested\n",
    "            sentences = []\n",
    "            if sentence_level:\n",
    "                for text in texts:\n",
    "                    sents = [sent.text for sent in nlp_token(text).sents]\n",
    "                    sentences.append(sents)\n",
    "            else:\n",
    "                sentences = [None] * len(texts)\n",
    "\n",
    "            return {\n",
    "                column + \"_tokens\": tokens,\n",
    "                column + \"_token_ids\": token_ids if token_ids else [None] * len(tokens),\n",
    "                column + \"_sentences\": sentences\n",
    "            }\n",
    "\n",
    "        tokenized_dataset = dataset.map(process_batch, batched=True)\n",
    "        if display:\n",
    "            print(f\"[INFO] Tokenization completed on Hugging Face dataset for column '{column}'.\")\n",
    "        return tokenized_dataset\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type. Use a Hugging Face dataset or pandas DataFrame.\")\n",
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Load SentenceTransformer Model (small but powerful)\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load spaCy model with vectors (make sure you have en_core_web_md installed)\n",
    "nlp_vec = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def encode_words(dataset, column=\"text\", batch_size=32, display=False):\n",
    "    \"\"\"\n",
    "    Encodes words individually into embeddings using spaCy's word vectors.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (Dataset, DatasetDict, or pd.DataFrame): The dataset to process.\n",
    "        column (str): The column containing text.\n",
    "        batch_size (int): Placeholder for compatibility (unused here).\n",
    "        display (bool): If True, prints a final summary message after encoding.\n",
    "\n",
    "    Returns:\n",
    "        (processed_dataset, word_embeddings_dict)\n",
    "            processed_dataset: The dataset with an added column (e.g. \"text_word_embeddings\") containing lists of embeddings.\n",
    "            word_embeddings_dict: A dictionary mapping each encountered word to its embedding.\n",
    "    \"\"\"\n",
    "    word_embeddings_dict = {}\n",
    "\n",
    "    def get_word_embeddings(text):\n",
    "        doc = nlp_vec(text)\n",
    "        embeddings = []\n",
    "        for token in doc:\n",
    "            word = token.text\n",
    "            emb = token.vector  # Get spaCy's vector for this token\n",
    "            embeddings.append(emb)\n",
    "            word_embeddings_dict[word] = emb  # Update dictionary (duplicates get overwritten with identical vectors)\n",
    "        return embeddings\n",
    "\n",
    "    if isinstance(dataset, pd.DataFrame):\n",
    "        # Apply function to every row in the DataFrame column\n",
    "        dataset[column + \"_word_embeddings\"] = dataset[column].apply(get_word_embeddings)\n",
    "        if display:\n",
    "            print(f\"[INFO] DataFrame processing complete. Processed {len(dataset)} rows.\")\n",
    "            print(f\"[INFO] Vocabulary size (unique words embedded): {len(word_embeddings_dict)}\")\n",
    "        return dataset, word_embeddings_dict\n",
    "\n",
    "    elif isinstance(dataset, (Dataset, DatasetDict)):\n",
    "        def process_batch(batch):\n",
    "            embeddings_batch = []\n",
    "            for text in batch[column]:\n",
    "                word_embeds = get_word_embeddings(text)\n",
    "                embeddings_batch.append(word_embeds)\n",
    "            return {column + \"_word_embeddings\": embeddings_batch}\n",
    "\n",
    "        processed_dataset = dataset.map(process_batch, batched=True)\n",
    "        if display:\n",
    "            total = processed_dataset.num_rows if hasattr(processed_dataset, \"num_rows\") else \"unknown\"\n",
    "            print(f\"[INFO] Hugging Face dataset processing complete. Processed {total} rows.\")\n",
    "            print(f\"[INFO] Vocabulary size (unique words embedded): {len(word_embeddings_dict)}\")\n",
    "        return processed_dataset, word_embeddings_dict\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported dataset type. Use a Hugging Face dataset or pandas DataFrame.\")\n",
    "\n",
    "\n",
    "\n",
    "import fasttext\n",
    "\n",
    "# Load the FastText model using the absolute path\n",
    "fasttext_model = fasttext.load_model(\"C:/Users/bhall/cc.en.300.bin\")\n",
    "\n",
    "'''# Test: Get word vector for a word\n",
    "word_vector = fasttext_model.get_word_vector(\"hello\")\n",
    "print(word_vector[:10])  # Print first 10 values'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(fasttext_model, word_index, embedding_dim=300, display=False):\n",
    "    \"\"\"\n",
    "    Creates an embedding matrix using a pre-trained FastText model.\n",
    "\n",
    "    Args:\n",
    "        fasttext_model: Loaded FastText model.\n",
    "        word_index (dict): Tokenizer's word-to-index mapping.\n",
    "        embedding_dim (int): Dimensionality of FastText embeddings (default: 300).\n",
    "        display (bool): If True, prints a final summary message.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Embedding matrix of shape (vocab_size, embedding_dim).\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, index in word_index.items():\n",
    "        if index >= vocab_size:  # Ignore words exceeding vocab limit\n",
    "            continue\n",
    "        embedding_vector = fasttext_model.get_word_vector(word)\n",
    "        embedding_matrix[index] = embedding_vector  # Assign to matrix\n",
    "\n",
    "    if display:\n",
    "        print(f\"[INFO] Created embedding matrix with shape: {embedding_matrix.shape}\")\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def convert_to_tensors(dataset, columns, labels_column=None, display=False):\n",
    "    \"\"\"\n",
    "    Converts specified dataset columns to PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset, DatasetDict, or pd.DataFrame): The dataset to process.\n",
    "        columns (list): List of columns to convert to tensors.\n",
    "        labels_column (str, optional): Column containing labels (for supervised tasks).\n",
    "        display (bool, optional): If True, prints out summary info after conversion.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing tensors for each specified column.\n",
    "    \"\"\"\n",
    "    tensor_dict = {}\n",
    "\n",
    "    for col in columns:\n",
    "        data = dataset[col]\n",
    "\n",
    "        # If data is a pandas Series, force each element to be a list\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = data.apply(lambda x: list(x) if not isinstance(x, list) else x)\n",
    "            data = list(data)  # Convert Series into a list\n",
    "\n",
    "        if isinstance(data, (list, np.ndarray)):\n",
    "            if col.endswith(\"_token_ids\"):\n",
    "                data = torch.tensor(data, dtype=torch.long)\n",
    "            else:\n",
    "                data = torch.tensor(data, dtype=torch.float32)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported data type for column: {col}\")\n",
    "\n",
    "        tensor_dict[col] = data\n",
    "\n",
    "    if labels_column:\n",
    "        if hasattr(dataset[labels_column], \"values\"):\n",
    "            labels = torch.tensor(dataset[labels_column].values, dtype=torch.long)\n",
    "        else:\n",
    "            labels = torch.tensor(dataset[labels_column], dtype=torch.long)\n",
    "        tensor_dict[\"labels\"] = labels\n",
    "\n",
    "    if display:\n",
    "        print(\"[INFO] Conversion to tensors complete. Summary:\")\n",
    "        for key, tensor in tensor_dict.items():\n",
    "            print(f\"  {key}: shape = {tensor.shape}\")\n",
    "\n",
    "    return tensor_dict\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, tensor_dict):\n",
    "        \"\"\"\n",
    "        Custom PyTorch Dataset for text-based data.\n",
    "\n",
    "        Args:\n",
    "            tensor_dict (dict): Dictionary containing tensors (including optional labels).\n",
    "        \"\"\"\n",
    "        self.data = tensor_dict\n",
    "        self.keys = list(tensor_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        # Assumes all tensor entries have the same length\n",
    "        return len(self.data[self.keys[0]])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {key: self.data[key][idx] for key in self.keys}\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloader(tensor_dict, batch_size=32, shuffle=True, display=False):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for batch processing.\n",
    "\n",
    "    Args:\n",
    "        tensor_dict (dict): Dictionary of tensors (from convert_to_tensors).\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        shuffle (bool): Whether to shuffle data.\n",
    "        display (bool, optional): If True, prints out summary info after creation.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader object.\n",
    "    \"\"\"\n",
    "    dataset = CustomTextDataset(tensor_dict)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    if display:\n",
    "        num_samples = len(dataset)\n",
    "        num_batches = len(dataloader)\n",
    "        print(f\"[INFO] DataLoader created: {num_samples} samples, batch size = {batch_size}, total batches = {num_batches}\")\n",
    "\n",
    "    return dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62474f38-d110-4715-a931-29f71970bf02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch GPU)",
   "language": "python",
   "name": "pytorch_env_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
